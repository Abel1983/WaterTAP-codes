{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e617b6c2",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# --- 0) Install (if needed) ---\n",
    "# !pip install torchgeo segmentation-models-pytorch pystac-client planetary-computer rioxarray rasterio ipywidgets\n",
    "\n",
    "# --- 1) Imports & Config ---\n",
    "import os, json, tempfile, uuid, math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "import pystac_client\n",
    "import planetary_computer\n",
    "import rioxarray as rxr\n",
    "import rasterio\n",
    "from rasterio.windows import from_bounds\n",
    "from torchgeo.datasets import RasterDataset, stack_samples\n",
    "from torchgeo.samplers import GridGeoSampler, RandomGeoSampler\n",
    "from segmentation_models_pytorch.encoders import get_preprocessing_fn\n",
    "from segmentation_models_pytorch.unet import Unet\n",
    "import kornia.augmentation as K\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54772240",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Config\n",
    "AOI_GEOJSON = \"aoi.geojson\"            # polygon in EPSG:4326\n",
    "DATE_RANGE = \"2024-06-01/2024-06-30\"\n",
    "BANDS = [\"B02\", \"B03\", \"B04\", \"B08\"]\n",
    "OUT_TIF = \"data/s2_aoi.tif\"\n",
    "LABELS_PARQUET = \"data/labels.parquet\"  # patch-level labels\n",
    "PATCH_SIZE = 256                       # pixels\n",
    "STRIDE = 256\n",
    "CLASSES = [\"water\", \"vegetation\", \"urban\", \"bare_soil\"]\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "os.makedirs(\"data\", exist_ok=True)\n",
    "\n",
    "# --- 2) Download Sentinel-2 L2A (least-cloudy) ---\n",
    "def download_s2(aoi_path, out_path):\n",
    "    with open(aoi_path) as f:\n",
    "        geom = json.load(f)[\"features\"][0][\"geometry\"]\n",
    "    client = pystac_client.Client.open(\"https://planetarycomputer.microsoft.com/api/stac/v1\")\n",
    "    search = client.search(\n",
    "        collections=[\"sentinel-2-l2a\"],\n",
    "        intersects=geom,\n",
    "        datetime=DATE_RANGE,\n",
    "        query={\"eo:cloud_cover\": {\"lt\": 20}},\n",
    "        max_items=1,\n",
    "    )\n",
    "    item = next(search.get_items())\n",
    "    signed = planetary_computer.sign(item)\n",
    "    assets = [signed.assets[b] for b in BANDS]\n",
    "    xr = rxr.open_rasterio([a.href for a in assets]).rio.clip([geom], from_disk=True)\n",
    "    xr.rio.to_raster(out_path)\n",
    "    print(f\"Saved {out_path}\")\n",
    "\n",
    "if not os.path.exists(OUT_TIF):\n",
    "    download_s2(AOI_GEOJSON, OUT_TIF)\n",
    "\n",
    "# --- 3) TorchGeo datasets ---\n",
    "class S2Dataset(RasterDataset):\n",
    "    filename_glob = \"*.tif\"\n",
    "    is_image = True\n",
    "    separate_files = False\n",
    "    all_bands = BANDS\n",
    "    rgb_bands = [\"B04\", \"B03\", \"B02\"]\n",
    "\n",
    "img_ds = S2Dataset(root=\"data\")\n",
    "\n",
    "# --- 4) Patch sampler (for unlabeled sampling & later prediction) ---\n",
    "sampler = GridGeoSampler(img_ds, size=PATCH_SIZE, stride=STRIDE)\n",
    "\n",
    "# --- 5) Simple labeling UI (patch-level class) ---\n",
    "def load_chip(sample):\n",
    "    arr = sample[\"image\"]  # (C, H, W)\n",
    "    # simple stretch for RGB preview\n",
    "    rgb_idx = [BANDS.index(b) for b in [\"B04\", \"B03\", \"B02\"]]\n",
    "    rgb = np.stack([arr[i] for i in rgb_idx], axis=-1)\n",
    "    rgb = np.clip((rgb - np.percentile(rgb, 2)) / (np.percentile(rgb, 98) - np.percentile(rgb, 2)), 0, 1)\n",
    "    return rgb\n",
    "\n",
    "def label_session(num_samples=20):\n",
    "    loader = DataLoader(img_ds, batch_sampler=sampler, collate_fn=stack_samples)\n",
    "    it = iter(loader)\n",
    "    records = []\n",
    "    class_dd = widgets.Dropdown(options=CLASSES, description=\"Class:\")\n",
    "    save_btn = widgets.Button(description=\"Save label\", button_style=\"success\")\n",
    "    status = widgets.Label()\n",
    "\n",
    "    def on_save(_):\n",
    "        nonlocal current_sample\n",
    "        rec = {\n",
    "            \"uuid\": str(uuid.uuid4()),\n",
    "            \"class\": class_dd.value,\n",
    "            \"bounds\": current_sample[\"bbox\"],\n",
    "            \"path\": current_sample[\"crs\"],\n",
    "            # store chip raw array path? we keep index only; model training uses bbox via sampler again\n",
    "        }\n",
    "        records.append(rec)\n",
    "        status.value = f\"Saved {len(records)} / {num_samples}\"\n",
    "        show_next()\n",
    "\n",
    "    def show_next():\n",
    "        nonlocal current_sample\n",
    "        try:\n",
    "            current_sample = next(it)\n",
    "        except StopIteration:\n",
    "            status.value = \"No more samples\"\n",
    "            return\n",
    "        rgb = load_chip(current_sample)\n",
    "        plt.figure(figsize=(4,4))\n",
    "        plt.imshow(rgb)\n",
    "        plt.axis(\"off\")\n",
    "        clear_output(wait=True)\n",
    "        display(class_dd, save_btn, status)\n",
    "        plt.show()\n",
    "\n",
    "    save_btn.on_click(on_save)\n",
    "    current_sample = None\n",
    "    show_next()\n",
    "    return records\n",
    "\n",
    "# Run labeling UI in notebook to collect labels\n",
    "# labeled = label_session(num_samples=30)\n",
    "\n",
    "# After labeling, save labels (bbox + class)\n",
    "def save_labels(records, path=LABELS_PARQUET):\n",
    "    df = pd.DataFrame(records)\n",
    "    df.to_parquet(path)\n",
    "    print(f\"Saved labels to {path}\")\n",
    "\n",
    "# --- 6) Training dataset using saved labels ---\n",
    "class PatchLabelDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, raster_ds, labels_df, patch_size=PATCH_SIZE):\n",
    "        self.raster_ds = raster_ds\n",
    "        self.labels = labels_df.reset_index(drop=True)\n",
    "        self.patch_size = patch_size\n",
    "        self.band_idx = list(range(len(BANDS)))\n",
    "        self.preproc = None  # could add normalization\n",
    "        self.aug = K.AugmentationSequential(\n",
    "            K.RandomHorizontalFlip(),\n",
    "            K.RandomVerticalFlip(),\n",
    "            data_keys=[\"input\"]\n",
    "        )\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.labels.iloc[idx]\n",
    "        bbox = row[\"bounds\"]\n",
    "        # bbox is [minx, miny, maxx, maxy]; use window read\n",
    "        with rasterio.open(self.raster_ds.filepaths[0]) as src:\n",
    "            window = from_bounds(*bbox, transform=src.transform)\n",
    "            chip = src.read(window=window, boundless=True, fill_value=0)\n",
    "        x = torch.from_numpy(chip.astype(np.float32))\n",
    "        sample = {\"input\": x}\n",
    "        sample = self.aug(sample)\n",
    "        x = sample[\"input\"]\n",
    "        y = torch.tensor(CLASSES.index(row[\"class\"]), dtype=torch.long)\n",
    "        return x, y\n",
    "\n",
    "# --- 7) Train classifier (patch-level) ---\n",
    "def train_model(labels_path=LABELS_PARQUET, epochs=5, batch_size=8):\n",
    "    labels_df = pd.read_parquet(labels_path)\n",
    "    ds = PatchLabelDataset(img_ds, labels_df)\n",
    "    dl = DataLoader(ds, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "    model = Unet(encoder_name=\"resnet34\", in_channels=len(BANDS), classes=len(CLASSES), encoder_weights=None)\n",
    "    # use global pooling to make classifier: replace final conv to logits, pool spatially\n",
    "    model.segmentation_head = nn.Identity()\n",
    "    clf_head = nn.Linear(64, len(CLASSES))  # 64 is decoder last channels for resnet34 UNet; adjust if needed\n",
    "\n",
    "    optimizer = torch.optim.Adam(list(model.parameters()) + list(clf_head.parameters()), lr=1e-3)\n",
    "    model.to(DEVICE); clf_head.to(DEVICE)\n",
    "    for epoch in range(epochs):\n",
    "        model.train(); clf_head.train()\n",
    "        losses = []\n",
    "        for x, y in dl:\n",
    "            x, y = x.to(DEVICE), y.to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            feats = model(x)              # (B, C, H, W) features\n",
    "            pooled = F.adaptive_avg_pool2d(feats, 1).squeeze(-1).squeeze(-1)  # (B, C)\n",
    "            logits = clf_head(pooled)\n",
    "            loss = F.cross_entropy(logits, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            losses.append(loss.item())\n",
    "        print(f\"Epoch {epoch+1}: loss={np.mean(losses):.4f}\")\n",
    "    return model, clf_head\n",
    "\n",
    "# --- 8) Sliding-window prediction ---\n",
    "def predict_map(model, clf_head, raster_path=OUT_TIF, out_csv=\"data/predictions.csv\"):\n",
    "    model.eval(); clf_head.eval()\n",
    "    preds = []\n",
    "    loader = DataLoader(img_ds, batch_sampler=sampler, collate_fn=stack_samples)\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            x = batch[\"image\"].to(DEVICE)\n",
    "            feats = model(x)\n",
    "            pooled = F.adaptive_avg_pool2d(feats, 1).squeeze(-1).squeeze(-1)\n",
    "            logits = clf_head(pooled)\n",
    "            prob = F.softmax(logits, dim=1).cpu().numpy()\n",
    "            for p, bbox in zip(prob, batch[\"bbox\"]):\n",
    "                preds.append({\"bbox\": bbox, **{f\"prob_{c}\": p[i] for i,c in enumerate(CLASSES)}})\n",
    "    pd.DataFrame(preds).to_csv(out_csv, index=False)\n",
    "    print(f\"Saved predictions (per-patch) to {out_csv}\")\n",
    "\n",
    "# --- 9) Usage flow (inside notebook) ---\n",
    "# 1. labeled = label_session(num_samples=30)\n",
    "# 2. save_labels(labeled)\n",
    "# 3. model, head = train_model(epochs=5, batch_size=8)\n",
    "# 4. predict_map(model, head)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
